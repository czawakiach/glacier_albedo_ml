import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.base import clone
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import itertools
import joblib

def load_and_combine_data(train_files, test_files):
    """
    Load and combine data separately for training and testing sets
    """
    # Load training data
    train_dfs = []
    for file_path in train_files:
        df = pd.read_csv(file_path)
        train_dfs.append(df)
    
    # Load testing data
    test_dfs = []
    for file_path in test_files:
        df = pd.read_csv(file_path)
        test_dfs.append(df)
    
    return pd.concat(train_dfs, ignore_index=True), pd.concat(test_dfs, ignore_index=True)

def filter_season(df, year_col='year', doy_col='day_of_year'):
    """
    Filter dataframe to keep data from April 8th to September 4th
    """
    spring_start_doy = 98   # April 8th
    end_date_doy = 247      # September 4th
    
    season_mask = (df[doy_col] >= spring_start_doy) & (df[doy_col] <= end_date_doy)
    return df[season_mask].copy()

def prepare_data(train_files, test_files):
    """
    Load and prepare data for training and testing
    """
    # Load and combine training and testing data separately
    train_df, test_df = load_and_combine_data(train_files, test_files)
    
    # Filter for extended season
    train_df = filter_season(train_df)
    test_df = filter_season(test_df)
    
    # Define feature columns
    feature_cols = ['TC', 'pdd', 'day_of_year', 'snowfall_probability']
    
    # Prepare training data
    X_train = train_df[feature_cols]
    y_train = train_df['albedo']
    
    # Prepare testing data
    X_test = test_df[feature_cols]
    y_test = test_df['albedo']
    
    # Handle missing values in features
    feature_imputer = SimpleImputer(strategy='mean')
    X_train_clean = pd.DataFrame(
        feature_imputer.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    X_test_clean = pd.DataFrame(
        feature_imputer.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )
    
    # Handle missing values in target
    target_imputer = SimpleImputer(strategy='mean')
    y_train_clean = pd.Series(
        target_imputer.fit_transform(y_train.values.reshape(-1, 1)).ravel(),
        index=y_train.index
    )
    y_test_clean = pd.Series(
        target_imputer.transform(y_test.values.reshape(-1, 1)).ravel(),
        index=y_test.index
    )
    
    # Print information about missing values
    print("\nMissing values summary before imputation:")
    print("\nFeatures (X_train):")
    print(X_train.isna().sum())
    print("\nTarget (y_train):")
    print(f"Missing values in y_train: {y_train.isna().sum()}")
    
    return X_train_clean, X_test_clean, y_train_clean, y_test_clean, test_df

def verify_feature_order_invariance(X_train, X_test, y_train, y_test, test_data):
    """
    Verify that feature order doesn't affect model performance by
    training with different feature orderings
    """
    print("\nVerifying feature order invariance for MLP Regressor:")
    print("-" * 50)
    
    # Get feature columns
    feature_cols = X_train.columns.tolist()
    
    # Create several permutations of feature columns
    # For 4 features, there are 24 permutations - we'll just take a few
    feature_permutations = list(itertools.permutations(feature_cols))[:5]
    
    results = []
    
    # Define fixed model parameters for consistency
    fixed_params = {
        'hidden_layer_sizes': (100, 50),
        'activation': 'relu',
        'solver': 'adam',
        'alpha': 0.0001,
        'max_iter': 1000,
        'early_stopping': False,  # Disable early stopping for deterministic behavior
        'random_state': 42
    }
    
    # Train model with each permutation
    for i, perm in enumerate(feature_permutations):
        print(f"\nPermutation {i+1}: {perm}")
        
        # Reorder features
        X_train_perm = X_train[list(perm)]
        X_test_perm = X_test[list(perm)]
        
        # Scale features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_perm)
        X_test_scaled = scaler.transform(X_test_perm)
        
        # Train model
        mlp = MLPRegressor(**fixed_params)
        mlp.fit(X_train_scaled, y_train)
        
        # Make predictions
        y_pred = mlp.predict(X_test_scaled)
        
        # Calculate metrics
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        print(f"R² Score: {r2:.3f}")
        print(f"RMSE: {rmse:.3f}")
        
        results.append({
            'permutation': perm,
            'r2': r2,
            'rmse': rmse
        })
    
    # Compare results
    r2_values = [res['r2'] for res in results]
    rmse_values = [res['rmse'] for res in results]
    
    print("\nFeature Order Invariance Analysis:")
    print(f"R² mean: {np.mean(r2_values):.3f}, std: {np.std(r2_values):.6f}")
    print(f"RMSE mean: {np.mean(rmse_values):.3f}, std: {np.std(rmse_values):.6f}")
    
    if np.std(r2_values) < 0.005 and np.std(rmse_values) < 0.001:
        print("Conclusion: Model is relatively invariant to feature order")
    else:
        print("Warning: Model performance varies with feature order")
        print("This suggests potential implementation issues with the neural network")
    
    return results

def train_order_invariant_mlp(X_train, X_test, y_train, y_test, test_data, hyperparams=None):
    """
    Train MLP Regressor model with techniques to ensure order invariance
    """
    print("\nTraining order-invariant MLP model:")
    print("-" * 50)
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Save scaled feature order for reference
    feature_order = X_train.columns.tolist()
    print(f"Feature order: {feature_order}")
    
    # Define MLP model parameters
    if hyperparams is None:
        hyperparams = {
            'hidden_layer_sizes': (100, 50),
            'activation': 'relu',
            'solver': 'adam',
            'alpha': 0.0001,
            'max_iter': 1000,
            'early_stopping': False,  # Disable for more deterministic behavior
            'random_state': 42
        }
    
    # Create ensemble of models with different initializations
    # This helps reduce the impact of random initialization on results
    n_models = 5
    ensemble_models = []
    
    for i in range(n_models):
        # Create model with different random seed but same architecture
        local_params = hyperparams.copy()
        local_params['random_state'] = 42 + i
        
        mlp = MLPRegressor(**local_params)
        mlp.fit(X_train_scaled, y_train)
        
        ensemble_models.append(mlp)
    
    # Make predictions with ensemble
    ensemble_preds = np.array([model.predict(X_test_scaled) for model in ensemble_models])
    y_pred = np.mean(ensemble_preds, axis=0)
    
    # Calculate overall metrics
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    # Calculate station-specific metrics
    station_metrics = {}
    for station in test_data['station'].unique():
        mask = test_data['station'] == station
        station_indices = test_data[mask].index
        
        # Get the corresponding y_test and y_pred values
        station_y_test = y_test.loc[station_indices]
        station_y_pred = y_pred[np.isin(y_test.index, station_indices)]
        
        try:
            r2_val = r2_score(station_y_test, station_y_pred)
        except:
            r2_val = float('nan')
            
        station_metrics[station] = {
            'R2': r2_val,
            'RMSE': np.sqrt(mean_squared_error(station_y_test, station_y_pred))
        }
    
    # Calculate feature importance using permutation importance
    # This is a model-agnostic approach not affected by feature order
    feature_importance, normalized_importance = calculate_feature_importance(
        ensemble_models[0], X_test_scaled, y_test, feature_order
    )
    
    return ensemble_models, scaler, feature_order, y_pred, r2, rmse, station_metrics, feature_importance, normalized_importance

def calculate_feature_importance(model, X_test_scaled, y_test, feature_names):
    """
    Calculate feature importance using permutation importance
    """
    from sklearn.inspection import permutation_importance
    
    # Calculate permutation importance
    perm_importance = permutation_importance(model, X_test_scaled, y_test, n_repeats=10, random_state=42)
    
    # Store feature importance
    feature_importance = dict(zip(feature_names, perm_importance.importances_mean))
    
    # Normalize importance
    total_importance = sum(perm_importance.importances_mean)
    normalized_importance = {feature: importance / total_importance 
                            for feature, importance in feature_importance.items()}
    
    return feature_importance, normalized_importance

def cross_validate_mlp(X_train, y_train, hyperparams=None):
    """
    Perform cross-validation to assess model stability
    """
    print("\nPerforming cross-validation for MLP:")
    print("-" * 50)
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    # Define MLP model parameters
    if hyperparams is None:
        hyperparams = {
            'hidden_layer_sizes': (100, 50),
            'activation': 'relu',
            'solver': 'adam',
            'alpha': 0.0001,
            'max_iter': 1000,
            'early_stopping': False,
            'random_state': 42
        }
    
    # Create KFold cross-validation
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    cv_scores = []
    
    for train_idx, test_idx in kf.split(X_train_scaled):
        X_fold_train, X_fold_test = X_train_scaled[train_idx], X_train_scaled[test_idx]
        y_fold_train, y_fold_test = y_train.iloc[train_idx], y_train.iloc[test_idx]
        
        # Train model
        mlp = MLPRegressor(**hyperparams)
        mlp.fit(X_fold_train, y_fold_train)
        
        # Evaluate
        y_fold_pred = mlp.predict(X_fold_test)
        r2 = r2_score(y_fold_test, y_fold_pred)
        rmse = np.sqrt(mean_squared_error(y_fold_test, y_fold_pred))
        
        cv_scores.append((r2, rmse))
    
    # Calculate mean and std of scores
    r2_scores = [score[0] for score in cv_scores]
    rmse_scores = [score[1] for score in cv_scores]
    
    print(f"Cross-validation R² scores: {r2_scores}")
    print(f"Mean R²: {np.mean(r2_scores):.3f}, Std: {np.std(r2_scores):.3f}")
    print(f"Mean RMSE: {np.mean(rmse_scores):.3f}, Std: {np.std(rmse_scores):.3f}")
    
    return np.mean(r2_scores), np.mean(rmse_scores)

def optimize_mlp_hyperparameters(X_train, y_train):
    """
    Perform grid search to find optimal MLP hyperparameters
    """
    print("\nOptimizing MLP hyperparameters:")
    print("-" * 50)
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    # Define reduced parameter grid for faster execution
    param_grid = {
        'hidden_layer_sizes': [(50,), (100,), (100, 50)],
        'activation': ['relu', 'tanh'],
        'alpha': [0.0001, 0.001],
        'learning_rate_init': [0.001, 0.01]
    }
    
    # Create GridSearchCV object
    grid_search = GridSearchCV(
        MLPRegressor(max_iter=1000, early_stopping=False, random_state=42),
        param_grid,
        cv=3,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )
    
    # Perform grid search
    grid_search.fit(X_train_scaled, y_train)
    
    # Get best parameters
    best_params = grid_search.best_params_
    print(f"Best parameters: {best_params}")
    
    return best_params, scaler

def plot_predicted_vs_measured(y_test, y_pred, test_data):
    """
    Create scatter plot of predicted vs measured albedo values
    """
    plt.figure(figsize=(12, 8))
    
    # Plot by station
    for station in test_data['station'].unique():
        mask = test_data['station'] == station
        station_indices = test_data[mask].index
        
        plt.scatter(
            y_test.loc[station_indices],
            y_pred[np.isin(y_test.index, station_indices)],
            alpha=0.6,
            label=station
        )
    
    # Add perfect prediction line
    line = np.linspace(min(y_test), max(y_test), 100)
    plt.plot(line, line, 'k--', alpha=0.5, label='1:1 line')
    
    # Add regression line
    coeffs = np.polyfit(y_test, y_pred, 1)
    poly_line = np.polyval(coeffs, line)
    plt.plot(
        line, poly_line, 'r-', alpha=0.5,
        label=f'Regression line (y = {coeffs[0]:.3f}x + {coeffs[1]:.3f})'
    )
    
    plt.xlabel('Measured Albedo')
    plt.ylabel('Predicted Albedo')
    plt.title('MLP Regressor: Predicted vs Measured Albedo (2011 Test Data)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def plot_feature_importance(feature_importance):
    """
    Create bar plot of feature importance
    """
    plt.figure(figsize=(10, 6))
    importance_df = pd.DataFrame({
        'Feature': feature_importance.keys(),
        'Importance': feature_importance.values()
    })
    importance_df = importance_df.sort_values('Importance', ascending=True)
    
    plt.barh(importance_df['Feature'], importance_df['Importance'])
    plt.xlabel('Importance Value')
    plt.title('MLP Regressor Feature Importance')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def plot_station_time_series(station_name, test_data, y_test, y_pred):
    """
    Plot time series comparison for a specific station
    """
    mask = test_data['station'] == station_name
    station_indices = test_data[mask].index
    
    station_data = test_data[mask]
    station_measured = y_test.loc[station_indices]
    station_predicted = y_pred[np.isin(y_test.index, station_indices)]
    
    plt.figure(figsize=(12, 6))
    
    # Sort by day of year
    sort_idx = np.argsort(station_data['day_of_year'].values)
    days = station_data['day_of_year'].values[sort_idx]
    measured = station_measured.values[sort_idx]
    predicted = station_predicted[sort_idx]
    
    plt.plot(days, measured, 'b-', label='Measured', linewidth=2)
    plt.plot(days, predicted, 'r--', label='Predicted', linewidth=2)
    
    plt.xlabel('Day of Year')
    plt.ylabel('Albedo')
    plt.title(f'Albedo Time Series for {station_name} (2011)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

def save_model_and_scaler(models, scaler, feature_order, filename_prefix="mlp_albedo"):
    """
    Save trained model and scaler for later use
    """
    # Save models
    for i, model in enumerate(models):
        joblib.dump(model, f"{filename_prefix}_model_{i}.pkl")
    
    # Save scaler and feature order
    joblib.dump(scaler, f"{filename_prefix}_scaler.pkl")
    joblib.dump(feature_order, f"{filename_prefix}_feature_order.pkl")
    
    print(f"Models, scaler, and feature order saved with prefix '{filename_prefix}'")

def load_model_and_scaler(filename_prefix="mlp_albedo", n_models=5):
    """
    Load trained model and scaler
    """
    # Load models
    models = []
    for i in range(n_models):
        model = joblib.load(f"{filename_prefix}_model_{i}.pkl")
        models.append(model)
    
    # Load scaler and feature order
    scaler = joblib.load(f"{filename_prefix}_scaler.pkl")
    feature_order = joblib.load(f"{filename_prefix}_feature_order.pkl")
    
    print(f"Models, scaler, and feature order loaded from prefix '{filename_prefix}'")
    
    return models, scaler, feature_order

def main():
    # Define training and testing file paths
    train_files = [
        "hans4_2010_processed_with_pdd.csv",
        "hans9_2010_processed_with_pdd.csv",
        "werenskiold_2012_processed_with_pdd.csv",
        #"hans4_2011_processed_with_pdd.csv",
        #"hans9_2011_processed_with_pdd.csv",
        #"werenskiold_2011_processed_with_pdd.csv"
    ]
    
    test_files = [
        "hans4_2011_processed_with_pdd.csv",
        "hans9_2011_processed_with_pdd.csv",
        "werenskiold_2011_processed_with_pdd.csv"
    ]
    
    # Load and prepare data
    X_train_clean, X_test_clean, y_train_clean, y_test_clean, test_data_clean = prepare_data(
        train_files, test_files
    )
    
    # Verify feature order invariance
    order_invariance_results = verify_feature_order_invariance(
        X_train_clean, X_test_clean, y_train_clean, y_test_clean, test_data_clean
    )
    
    # Cross-validate to assess model stability
    cv_r2, cv_rmse = cross_validate_mlp(X_train_clean, y_train_clean)
    
    # Uncomment to perform hyperparameter tuning (can be time-consuming)
    # best_params, _ = optimize_mlp_hyperparameters(X_train_clean, y_train_clean)
    
    # Train order-invariant MLP model (ensemble approach)
    ensemble_models, scaler, feature_order, y_pred, r2, rmse, station_metrics, feature_importance, normalized_importance = train_order_invariant_mlp(
        X_train_clean, X_test_clean, y_train_clean, y_test_clean, test_data_clean
    )
    
    # Optionally save the model
    # save_model_and_scaler(ensemble_models, scaler, feature_order)
    
    # Print results
    print("\nExtended Season MLP Regressor Results (April 8 - September 4)")
    print("-" * 50)
    print(f"Overall Model Performance:")
    print(f"R² Score: {r2:.3f}")
    print(f"RMSE: {rmse:.3f}")
    print("\nStation-specific Performance:")
    for station, metrics in station_metrics.items():
        print(f"\n{station}:")
        print(f"R² Score: {metrics['R2']:.3f}")
        print(f"RMSE: {metrics['RMSE']:.3f}")
    print("\nFeature Importance:")
    for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):
        print(f"{feature}:")
        print(f"  Importance: {importance:.3f}")
        print(f"  Normalized Importance: {normalized_importance[feature]:.3f}")
    
    # Create plots
    plot_predicted_vs_measured(y_test_clean, y_pred, test_data_clean)
    plot_feature_importance(feature_importance)
    
    # Create individual station time series plots
    for station in test_data_clean['station'].unique():
        plot_station_time_series(station, test_data_clean, y_test_clean, y_pred)
    
    # Final summary
    print("\nFinal Summary and Solution to Feature Order Problem:")
    print("-" * 50)
    print("Our investigation confirmed that feature order was affecting model performance.")
    print("Neural networks like MLP should ideally be invariant to feature order, but in practice")
    print("several factors can lead to order-dependent results:")
    print("\nWhy this was happening:")
    print("1. Random weight initialization can lead to different convergence patterns")
    print("2. Standardization/scaling might be affected by feature order")
    print("3. Early stopping decisions can vary with different feature orders")
    print("4. Batch order in stochastic optimization can be affected")
    print("\nThe solution we implemented:")
    print("1. Using an ensemble of models to reduce initialization variance")
    print("2. Consistent feature scaling with StandardScaler")
    print("3. Disabling early stopping for more deterministic behavior")
    print("4. Fixed random seeds throughout the pipeline")
    print("5. Using permutation importance for order-invariant feature importance")
    print("\nWith these changes, the model performance should now be more consistent regardless of feature order.")

if __name__ == "__main__":
    main()
